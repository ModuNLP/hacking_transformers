{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELECTRA on ğŸ¤— Transformers ğŸ¤—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.11.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¶ˆí•„ìš”í•œ ë¡œê¹… ë©”ì‹œì§€ ì œê±°ìš©\n",
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.WARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/28896432/80024445-0f444e00-851a-11ea-9137-9da2abfd553d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TL;DR (Example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Discriminator\n",
    "- [electra-base-discriminator](https://huggingface.co/google/electra-base-discriminator#how-to-use-the-discriminator-in-transformers)\n",
    "- Fake Token Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Park_Jang_Won\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\transformers\\tokenization_utils.py:831: FutureWarning: Parameter max_len is deprecated and will be removed in a future release. Use model_max_length instead.\n",
      "  category=FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ë‚˜ëŠ”', 0.0), ('ì™œ', 1.0), ('ë°¥ì„', 0.0), ('ë¨¹ì—ˆë‹¤', 0.0), ('.', 0.0)]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import ElectraForPreTraining, ElectraTokenizer\n",
    "from pprint import pprint\n",
    "\n",
    "discriminator = ElectraForPreTraining.from_pretrained(\"monologg/koelectra-base-discriminator\")\n",
    "tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-discriminator\")\n",
    "\n",
    "sentence = \"ë‚˜ëŠ” ë°©ê¸ˆ ë°¥ì„ ë¨¹ì—ˆë‹¤.\"\n",
    "fake_sentence = \"ë‚˜ëŠ” ì™œ ë°¥ì„ ë¨¹ì—ˆë‹¤.\"\n",
    "\n",
    "fake_tokens = tokenizer.tokenize(fake_sentence)\n",
    "fake_inputs = tokenizer.encode(fake_sentence, return_tensors=\"pt\")\n",
    "\n",
    "discriminator_outputs = discriminator(fake_inputs)\n",
    "predictions = torch.round((torch.sign(discriminator_outputs[0]) + 1) / 2)\n",
    "\n",
    "pprint(list(zip(fake_tokens, predictions.tolist()[1:-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generator\n",
    "\n",
    "- [electra-base-generator](https://huggingface.co/google/electra-base-generator#how-to-use-the-generator-in-transformers)\n",
    "- ê¸°ì¡´ BERTì˜ Mask Token Predictionê³¼ ë™ì¼í•˜ë‹¤ê³  ìƒê°í•˜ë©´ ë¨!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a37afcb376274353a0dcdc9df541a442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=463, style=ProgressStyle(description_width=â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f0bab4fb58342dc97d67b364f6c652e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=279173, style=ProgressStyle(description_widâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b20f94c8a8a343639705214492c92fd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=51, style=ProgressStyle(description_width='â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Park_Jang_Won\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\transformers\\tokenization_utils.py:831: FutureWarning: Parameter max_len is deprecated and will be removed in a future release. Use model_max_length instead.\n",
      "  category=FutureWarning,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cec86e2841ee48aabc43964c03edc668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=140170987, style=ProgressStyle(description_â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[{'score': 0.0713089108467102,\n",
      "  'sequence': '[CLS] ë‚˜ëŠ” ì‹ë‹¹ì—ì„œ ë°¥ì„ ë¨¹ì—ˆë‹¤. [SEP]',\n",
      "  'token': 26194},\n",
      " {'score': 0.04359067603945732,\n",
      "  'sequence': '[CLS] ë‚˜ëŠ” ë°©ê¸ˆ ë°¥ì„ ë¨¹ì—ˆë‹¤. [SEP]',\n",
      "  'token': 24499},\n",
      " {'score': 0.02970987744629383,\n",
      "  'sequence': '[CLS] ë‚˜ëŠ” ë‹¤ì‹œ ë°¥ì„ ë¨¹ì—ˆë‹¤. [SEP]',\n",
      "  'token': 10715},\n",
      " {'score': 0.027878431603312492,\n",
      "  'sequence': '[CLS] ë‚˜ëŠ” ì•‰ì•„ì„œ ë°¥ì„ ë¨¹ì—ˆë‹¤. [SEP]',\n",
      "  'token': 23755},\n",
      " {'score': 0.025679852813482285,\n",
      "  'sequence': '[CLS] ë‚˜ëŠ” ë‚´ ë°¥ì„ ë¨¹ì—ˆë‹¤. [SEP]',\n",
      "  'token': 783}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from pprint import pprint\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"monologg/koelectra-base-generator\",\n",
    "    tokenizer=\"monologg/koelectra-base-generator\"\n",
    ")\n",
    "\n",
    "pprint(fill_mask(\"ë‚˜ëŠ” {} ë°¥ì„ ë¨¹ì—ˆë‹¤.\".format(fill_mask.tokenizer.mask_token)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detail Review for Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pretrainingì˜ ê²½ìš°ëŠ” ì› ì €ìì˜ Tensorflow ì½”ë“œë¥¼ ì“°ëŠ” ê²ƒì„ ì¶”ì²œ\n",
    "- Huggingface Transformersì˜ ì½”ë“œëŠ” Pretrainingì´ ì™„ë£Œëœ ëª¨ë¸ì„ ê°€ì ¸ë‹¤ ì“°ëŠ” ìš©ë„\n",
    "- [modeling_electra.py](https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_electra.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers.modeling_bert import BertEmbeddings\n",
    "from transformers.modeling_electra import ElectraPreTrainedModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. ElectraEmbeddings\n",
    "\n",
    "BertEmbeddingsì™€ ë™ì¼!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElectraEmbeddings(BertEmbeddings):\n",
    "    \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.embedding_size, padding_idx=config.pad_token_id)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.embedding_size)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.embedding_size)\n",
    "\n",
    "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
    "        # any TensorFlow checkpoint file\n",
    "        self.LayerNorm = BertLayerNorm(config.embedding_size, eps=config.layer_norm_eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ElectraModel\n",
    "\n",
    "BertModelê³¼ ë™ì¼í•˜ì§€ë§Œ PoolerëŠ” ì—†ìŒ!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. ElectraForPreTraining\n",
    "\n",
    "- Electra model with a binary classification head on top as used during pre-training for identifying generated tokens.\n",
    "- ê°œì¸ì ìœ¼ë¡œ ì´ í´ë˜ìŠ¤ ì´ë¦„ì„ ì¢‹ì•„í•˜ì§„ ì•ŠìŒ\n",
    "- ë…¼ë¬¸ì—ì„œëŠ” `ElectraModel`ë„ discriminatorì˜ ê²ƒì„ ì‚¬ìš©í•˜ë¼ê³  í•˜ê³  ìˆìŒ\n",
    "\n",
    "```python\n",
    "model = ElectraForPreTraining.from_pretrained(\"google/electra-base-discriminator\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ElectraForPreTraining\n",
    "\n",
    "model = ElectraForPreTraining.from_pretrained(\"google/electra-base-discriminator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `nn.Linear(config.hidden_size, 1)`ë¥¼ í†µê³¼í•œ í›„, `BCEWithLogitsLoss`ë¥¼ ì‚¬ìš©í•˜ì—¬ Sigmoid ì ìš©!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElectraDiscriminatorPredictions(nn.Module):\n",
    "    \"\"\"Prediction module for the discriminator, made up of two dense layers.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dense_prediction = nn.Linear(config.hidden_size, 1)\n",
    "        self.config = config\n",
    "\n",
    "    def forward(self, discriminator_hidden_states, attention_mask):\n",
    "        hidden_states = self.dense(discriminator_hidden_states)\n",
    "        hidden_states = get_activation(self.config.hidden_act)(hidden_states)\n",
    "        logits = self.dense_prediction(hidden_states).squeeze()\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElectraForPreTraining(ElectraPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.electra = ElectraModel(config)\n",
    "        self.discriminator_predictions = ElectraDiscriminatorPredictions(config)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "    ):\n",
    "        discriminator_hidden_states = self.electra(\n",
    "            input_ids,\n",
    "            attention_mask,\n",
    "            token_type_ids,\n",
    "            position_ids,\n",
    "            head_mask,\n",
    "            inputs_embeds,\n",
    "            output_attentions,\n",
    "            output_hidden_states,\n",
    "        )\n",
    "        discriminator_sequence_output = discriminator_hidden_states[0]\n",
    "\n",
    "        logits = self.discriminator_predictions(discriminator_sequence_output, attention_mask)\n",
    "\n",
    "        output = (logits,)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.BCEWithLogitsLoss()\n",
    "            if attention_mask is not None:\n",
    "                active_loss = attention_mask.view(-1, discriminator_sequence_output.shape[1]) == 1\n",
    "                active_logits = logits.view(-1, discriminator_sequence_output.shape[1])[active_loss]\n",
    "                active_labels = labels[active_loss]\n",
    "                loss = loss_fct(active_logits, active_labels.float())\n",
    "            else:\n",
    "                loss = loss_fct(logits.view(-1, discriminator_sequence_output.shape[1]), labels.float())\n",
    "\n",
    "            output = (loss,) + output\n",
    "\n",
    "        output += discriminator_hidden_states[1:]\n",
    "\n",
    "        return output  # (loss), scores, (hidden_states), (attentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. ElectraForMaskedLM\n",
    "\n",
    "- Electra model with a language modeling head on top.\n",
    "- ìš°ë¦¬ê°€ ì•„ëŠ” BERTì˜ Masked Token Prediction\n",
    "\n",
    "```python\n",
    "model = ElectraForMaskedLM.from_pretrained('google/electra-base-generator')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ElectraForMaskedLM\n",
    "\n",
    "model = ElectraForMaskedLM.from_pretrained('google/electra-base-generator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElectraGeneratorPredictions(nn.Module):\n",
    "    \"\"\"Prediction module for the generator, made up of two dense layers.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.LayerNorm = BertLayerNorm(config.embedding_size)\n",
    "        self.dense = nn.Linear(config.hidden_size, config.embedding_size)\n",
    "\n",
    "    def forward(self, generator_hidden_states):\n",
    "        hidden_states = self.dense(generator_hidden_states)\n",
    "        hidden_states = get_activation(\"gelu\")(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states)\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElectraForMaskedLM(ElectraPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.electra = ElectraModel(config)\n",
    "        self.generator_predictions = ElectraGeneratorPredictions(config)\n",
    "\n",
    "        self.generator_lm_head = nn.Linear(config.embedding_size, config.vocab_size)\n",
    "        self.init_weights()\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.generator_lm_head\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        if \"masked_lm_labels\" in kwargs:\n",
    "            warnings.warn(\n",
    "                \"The `masked_lm_labels` argument is deprecated and will be removed in a future version, use `labels` instead.\",\n",
    "                DeprecationWarning,\n",
    "            )\n",
    "            labels = kwargs.pop(\"masked_lm_labels\")\n",
    "        assert kwargs == {}, f\"Unexpected keyword arguments: {list(kwargs.keys())}.\"\n",
    "\n",
    "        generator_hidden_states = self.electra(\n",
    "            input_ids,\n",
    "            attention_mask,\n",
    "            token_type_ids,\n",
    "            position_ids,\n",
    "            head_mask,\n",
    "            inputs_embeds,\n",
    "            output_attentions,\n",
    "            output_hidden_states,\n",
    "        )\n",
    "        generator_sequence_output = generator_hidden_states[0]\n",
    "\n",
    "        prediction_scores = self.generator_predictions(generator_sequence_output)\n",
    "        prediction_scores = self.generator_lm_head(prediction_scores)\n",
    "\n",
    "        output = (prediction_scores,)\n",
    "\n",
    "        # Masked language modeling softmax layer\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()  # -100 index = padding token\n",
    "            loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "            output = (loss,) + output\n",
    "\n",
    "        output += generator_hidden_states[1:]\n",
    "\n",
    "        return output  # (masked_lm_loss), prediction_scores, (hidden_states), (attentions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
